{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'test/test0.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1a68ab2bb06c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m                 \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test/test'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/envs/custom_keras/lib/python2.7/site-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2247\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2248\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2249\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2251\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'test/test0.png'"
     ]
    }
   ],
   "source": [
    "'''Train a Siamese MLP on pairs of digits from the MNIST dataset.\n",
    "This script uses a generator to load chunks of data into memory for training instead of \n",
    "the entire data set. This is useful in situations where the data set does not fit into\n",
    "memory.\n",
    "It follows Hadsell-et-al.'06 [1] by computing the Euclidean distance on the\n",
    "output of the shared network and by optimizing the contrastive loss (see paper\n",
    "for mode details).\n",
    "[1] \"Dimensionality Reduction by Learning an Invariant Mapping\"\n",
    "\thttp://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_siamese_generator.py\n",
    "'''\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import random\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Input, Lambda\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "\tx, y = vects\n",
    "\treturn K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "\tshape1, shape2 = shapes\n",
    "\treturn shape1\n",
    "\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "\t'''Contrastive loss from Hadsell-et-al.'06\n",
    "\thttp://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "\t'''\n",
    "\tmargin = 1\n",
    "\treturn K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    json_string = model.to_json()\n",
    "    if not os.path.isdir('cache'):\n",
    "        os.mkdir('cache')\n",
    "    open(os.path.join('cache', 'architecture.json'), 'w').write(json_string)\n",
    "    model.save_weights(os.path.join('cache', 'model_weights.h5'), overwrite=True)\n",
    "\n",
    "\n",
    "def read_model():\n",
    "    model = model_from_json(open(os.path.join('cache', 'architecture.json')).read())\n",
    "    model.load_weights(os.path.join('cache', 'model_weights.h5'))\n",
    "    return model\n",
    "    \n",
    "def create_pairs(x, digit_indices):\n",
    "\t'''Positive and negative pair creation.\n",
    "\tAlternates between positive and negative pairs.\n",
    "\t'''\n",
    "\tpairs = []\n",
    "\tlabels = []\n",
    "\tn = min([len(digit_indices[d]) for d in range(10)]) - 1\n",
    "\tfor d in range(10):\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tz1, z2 = digit_indices[d][i], digit_indices[d][i+1]\n",
    "\t\t\tpairs += [[x[z1], x[z2]]]\n",
    "\t\t\tinc = random.randrange(1, 10)\n",
    "\t\t\tdn = (d + inc) % 10\n",
    "\t\t\tz1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
    "\t\t\tpairs += [[x[z1], x[z2]]]\n",
    "\t\t\tlabels += [1, 0]\n",
    "\treturn np.array(pairs), np.array(labels)\n",
    "\t\n",
    "def create_pairs2(x, digit_indices):\n",
    "\t'''Positive and negative pair creation.\n",
    "\tAlternates between positive and negative pairs.\n",
    "\t'''\n",
    "\n",
    "\tn_pairs = 2*len(np.concatenate(digit_indices))\n",
    "\tn_values = len(digit_indices)\n",
    "\tpairs = np.zeros((n_pairs, 2) + x.shape[1:])\n",
    "\tlabels = np.zeros(n_pairs)\n",
    "\t\n",
    "\tq = 0\n",
    "\twhile q < n_pairs/2:\n",
    "\t\ti = np.random.randint(n_values)\n",
    "\t\tif len(digit_indices[i]) <= 1:\n",
    "\t\t\tcontinue\n",
    "\t\tj,k = np.random.choice(digit_indices[i], replace=False, size=2)\n",
    "\t\tpairs[2*q, 0] = x[j]\n",
    "\t\tpairs[2*q, 1] = x[k]\n",
    "\t\tlabels[2*q] = 1\n",
    "\t\tq += 1\n",
    "\n",
    "\tq = 0\n",
    "\tuseable_indices = [i for i in range(n_values) if len(digit_indices[i]) > 1]\n",
    "\twhile q < n_pairs/2:\n",
    "\t\ti, i2 = np.random.choice(useable_indices, replace=False, size=2)\n",
    "\t\tj = np.random.choice(digit_indices[i], replace=False, size=1)\n",
    "\t\tk = np.random.choice(digit_indices[i2], replace=False, size=1)\n",
    "\t\tpairs[2*q+1, 0] = x[j]\n",
    "\t\tpairs[2*q+1, 1] = x[k]\n",
    "\t\tq += 1\t\t\n",
    "\n",
    "\treturn pairs, labels\n",
    "\n",
    "'''\n",
    "def create_all_pairs(x, labels):\n",
    "\tn = x.shape[0]\n",
    "\tall_pairs = np.zeros((n*(n-1)/2,2) + x.shape)\n",
    "\tfor i in xrange(n):\n",
    "\t\tfor j in xrange(i+1, n):\n",
    "\t\t\tall_pairs(n*i+j*i\n",
    "'''\t\t\n",
    "\n",
    "'''\n",
    "def create_base_network(input_dim):\n",
    "\t#Base network to be shared (eq. to feature extraction).\n",
    "\t\n",
    "\tseq = Sequential()\n",
    "\tseq.add(Dense(128, input_shape=(input_dim,), activation='relu'))\n",
    "\tseq.add(Dropout(0.1))\n",
    "\tseq.add(Dense(128, activation='relu'))\n",
    "\tseq.add(Dropout(0.1))\n",
    "\tseq.add(Dense(128, activation='relu'))\n",
    "\treturn seq\n",
    "'''\t\n",
    "\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "\t'''Compute classification accuracy with a fixed threshold on distances.\n",
    "\t'''\n",
    "\treturn labels[predictions.ravel() < 0.5].mean()\n",
    "\n",
    "\n",
    "def create_base_network(input_dim):\n",
    "\t# input image dimensions\n",
    "\timg_colours, img_rows, img_cols = input_dim\n",
    "\n",
    "\t# number of convolutional filters to use\n",
    "\tnb_filters = 32\n",
    "\t# size of pooling area for max pooling\n",
    "\tnb_pool = 2\n",
    "\t# convolution kernel size\n",
    "\tnb_conv = 3\n",
    "\tmodel = Sequential()\n",
    "\n",
    "\tmodel.add(Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "\t\t\t\t\t\t\tborder_mode='valid',\n",
    "\t\t\t\t\t\t\tinput_shape=(img_colours, img_rows, img_cols)))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "\t#model.add(Dropout(0.1)) #0.25 #too much dropout and loss -> nan\n",
    "\n",
    "\tmodel.add(Flatten())\n",
    "\t\n",
    "\tmodel.add(Dense(64, input_shape=(input_dim,), activation='relu'))\n",
    "\t#model.add(Dropout(0.05))\n",
    "\tmodel.add(Dense(32, activation='relu'))\n",
    "\n",
    "\t\n",
    "\t'''\n",
    "\tmodel.add(Dense(32)) #128\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(Dropout(0.1)) #0.5\n",
    "\tmodel.add(Dense(10, activation='tanh')) #128\n",
    "\t#model.add(Dense(10))\n",
    "\t#model.add(Activation('softmax'))\n",
    "\t'''\n",
    "\t\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def preprocess(X):\n",
    "\t#this preprocessor crops one pixel along each of the sides of the images\n",
    "\treturn X[:, :, 1:-1, 1:-1] / 255.0\t\n",
    "\t#return X/ 255.0\t\n",
    "\n",
    "\n",
    "def filereader(fname):\n",
    "\tx = np.array(Image.open(fname))\n",
    "\t\n",
    "\tif len(x.shape) == 2:\n",
    "\t\t#add an additional colour dimension if the only dimensions are width and height\n",
    "\t\treturn preprocess( x.reshape((1, 1) + x.shape) )\n",
    "\tif len(x.shape) == 3:\n",
    "\t\treturn preprocess( x.reshape((1) + x.shape) )\t\n",
    "\t\t\n",
    "\t#return np.array(Image.open('train/train'+str(n)+'.png'))[1:-1, 1:-1]\n",
    "\t\n",
    "\n",
    "def myGenerator(y_train, chunk_size, batch_size):\n",
    "\tposs_values = np.unique(y_train).astype(int)\n",
    "\t\n",
    "\t#read and preprocess first file to figure out the image dimensions\n",
    "\tsample_file = filereader('train/train'+str(0)+'.png')\n",
    "\tnew_img_colours, new_img_rows, new_img_cols = sample_file.shape\n",
    "\t\n",
    "\tpool = Pool(processes=16)\n",
    "\t\n",
    "\twhile 1:\n",
    "\t\tfor i in xrange(y_train.shape[0]/chunk_size):\n",
    "\t\t\tX_train = pool.map(filereader, ['train/train'+str(i*chunk_size+i2)+'.png'  for i2 in xrange(chunk_size)])\n",
    "\t\t\tX_train = np.array(X_train).reshape((chunk_size, new_img_colours, new_img_rows, new_img_cols)) #.astype('float32')\n",
    "\t\t\t\t\t\t\n",
    "\t\t\tfor j in xrange(int(chunk_size/batch_size)): \t\n",
    "\t\t\t\tdigit_indices = [np.where(y_train[i*chunk_size+j*batch_size:i*chunk_size+(j+1)*batch_size] == k)[0] for k in poss_values]\n",
    "\t\t\t\ttr_pairs, tr_y = create_pairs2(X_train[j*batch_size:(j+1)*batch_size], digit_indices)\n",
    "                print tr_pairs.shape, tr_y.shape\n",
    "\t\t\t\tyield [tr_pairs[:, 0], tr_pairs[:, 1]], tr_y\n",
    "\n",
    "\n",
    "\n",
    "def do_split():\n",
    "\tif os.path.isdir('train') and os.path.isdir('test'):\n",
    "\t\treturn\n",
    "\t\n",
    "\t(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\t\t\n",
    "\tos.mkdir('train')\n",
    "\tos.mkdir('test')\n",
    "\t\n",
    "\tnp.savetxt('labels_train.csv', y_train, header='label')\n",
    "\tnp.savetxt('labels_test.csv', y_test, header='label')\n",
    "\t\n",
    "\tfor i in xrange(X_train.shape[0]):\n",
    "\t\tim = Image.fromarray(np.uint8(X_train[i]))\n",
    "\t\tim.save('train'+str(i)+'.png')\n",
    "\t\n",
    "\tfor i in xrange(X_test.shape[0]):\n",
    "\t\tim = Image.fromarray(np.uint8(X_test[i]))\n",
    "\t\tim.save('test'+str(i)+'.png')\t\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#def fit_model():\t\n",
    "\t#unzip the mnist data into train and test directories and create labels_train.csv and labels_test.csv\n",
    "\tdo_split()\n",
    "\t\n",
    "\t#load all the labels for the train and test sets\n",
    "\ty_train = np.loadtxt('labels_train.csv')\n",
    "\ty_test = np.loadtxt('labels_test.csv')\n",
    "\n",
    "\t# input image dimensions\n",
    "\timg_rows, img_cols = 28, 28\n",
    "\n",
    "\tX_test = np.zeros((y_test.shape[0], 1, img_rows, img_cols))\n",
    "\tfor j in xrange(y_test.shape[0]):\n",
    "\t\tX_test[j] = np.array(Image.open('test/test'+str(j)+'.png'))\n",
    "\n",
    "\tX_test = X_test.reshape(-1, 1, img_rows, img_cols)\t\n",
    "\n",
    "\n",
    "\tnb_epoch = 12\n",
    "\tbatch_size = 32\n",
    "\tchunk_size = y_train.shape[0]/4\n",
    "\n",
    "\t# create pairs of images in the test set\n",
    "\tdigit_indices = [np.where(y_test == i)[0] for i in range(10)]\n",
    "\t#te_pairs, te_y = create_pairs(X_test, digit_indices)\n",
    "\tte_pairs, te_y = create_pairs(preprocess(X_test), digit_indices)\n",
    "\n",
    "\tnew_img_colours, new_img_rows, new_img_cols = te_pairs[0].shape[-3:]\n",
    "\n",
    "\t# network definition\n",
    "\t#base_network = create_base_network(input_dim)\n",
    "\tbase_network = create_base_network((new_img_colours, new_img_rows, new_img_cols))\n",
    "\n",
    "\tinput_a = Input(shape=(new_img_colours, new_img_rows, new_img_cols,))\n",
    "\tinput_b = Input(shape=(new_img_colours, new_img_rows, new_img_cols,))\n",
    "\n",
    "\t# because we re-use the same instance 'base_network',\n",
    "\t# the weights of the network\n",
    "\t# will be shared across the two branches\n",
    "\tprocessed_a = base_network(input_a)\n",
    "\tprocessed_b = base_network(input_b)\n",
    "\n",
    "\tdistance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "\tmodel = Model(input=[input_a, input_b], output=distance)\n",
    "\n",
    "\t# train\n",
    "\trms = RMSprop()\n",
    "\tmodel.compile(loss=contrastive_loss, optimizer=rms)\n",
    "\n",
    "\t'''\n",
    "\tmodel.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
    "\t\t\t  validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y),\n",
    "\t\t\t  batch_size=128,\n",
    "\t\t\t  nb_epoch=nb_epoch)\n",
    "\t'''\t\t  \n",
    "\t\t  \n",
    "\t\t  \n",
    "\tmodel.fit_generator(myGenerator(y_train, chunk_size, batch_size), samples_per_epoch = y_train.shape[0], nb_epoch = nb_epoch, verbose=2,callbacks=[], validation_data=None, class_weight=None) # show_accuracy=True, nb_worker=1 \n",
    "\t\t  \n",
    "\n",
    "\t# compute final accuracy on training and test sets\n",
    "\t#pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n",
    "\t#tr_acc = compute_accuracy(pred, tr_y)\n",
    "\t#print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
    "\n",
    "\tpred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\n",
    "\tte_acc = compute_accuracy(pred, te_y)\n",
    "\tprint('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
